

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Uncertainty sampling &mdash; modAL  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Disagreement sampling for committee-based sampling" href="Disagreement-sampling.html" />
    <link rel="prev" title="Acquisition functions for Bayesian optimization" href="Acquisition-functions.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/modAL_b.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview/modAL-in-a-nutshell.html">modAL in a nutshell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/Installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/Extending-modAL.html">Extending modAL</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../models/ActiveLearner.html">ActiveLearner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/BayesianOptimizer.html">BayesianOptimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/Committee.html">Committee</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/CommitteeRegressor.html">CommitteeRegressor</a></li>
</ul>
<p class="caption"><span class="caption-text">Query strategies</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Acquisition-functions.html">Acquisition functions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Uncertainty sampling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#classification-uncertainty">Classification uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="#classification-margin">Classification margin</a></li>
<li class="toctree-l2"><a class="reference internal" href="#classification-entropy">Classification entropy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Disagreement-sampling.html">Disagreement sampling</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/Pool-based-sampling.html">Pool-based sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Stream-based-sampling.html">Stream-based sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Active-regression.html">Active regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Ensemble-regression.html">Ensemble regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Bayesian-optimization.html">Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Query-by-committee.html">Query by committee</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Bootstrapping-and-bagging.html">Bootstrapping and bagging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Keras-integration.html">Keras integration</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">modAL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>Uncertainty sampling</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/content/query_strategies/Uncertainty-sampling.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="uncertainty-sampling">
<h1>Uncertainty sampling<a class="headerlink" href="#uncertainty-sampling" title="Permalink to this headline">¶</a></h1>
<p>When you present unlabelled examples to an active learner, it finds you the most <em>useful</em> example and presents it for you to be labelled. This is done by first calculating the <em>usefulness</em> of prediction (whatever it means) for each example and select an instance based on the usefulness. The thing is, there are several ways to measure this. They are based upon the classification uncertainty, hence they are called <em>uncertainty measures</em>. In modAL, currently you can select from three built-in measures: <em>classification uncertainty</em>, <em>classification margin</em> and <em>classification entropy</em>. In this quick tutorial, we are going to review them. For more details, see Section 2.3 of the awesome book <a class="reference external" href="http://active-learning.net/">Active learning by Burr Settles</a>.</p>
<div class="section" id="classification-uncertainty">
<h2>Classification uncertainty<a class="headerlink" href="#classification-uncertainty" title="Permalink to this headline">¶</a></h2>
<p>The simplest measure is the uncertainty of classification defined by</p>
<div class="math notranslate nohighlight">
\[U(x)=1-P(\hat{x}|x)\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the instance to be predicted and <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the most likely prediction.</p>
<p>For example, if you have classes <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2]</span></code> and classification probabilities <code class="docutils literal notranslate"><span class="pre">[0.1,</span> <span class="pre">0.2,</span> <span class="pre">0.7]</span></code>, the most likely class according to the classifier is <code class="docutils literal notranslate"><span class="pre">2</span></code> with uncertainty 0.3. If you have three instances with class probabilities</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">proba</span>
<span class="gp">... </span><span class="p">[[</span><span class="mf">0.1</span> <span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>
<span class="gp">... </span> <span class="p">[</span><span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.3</span> <span class="p">,</span> <span class="mf">0.1</span> <span class="p">]</span>
<span class="gp">... </span> <span class="p">[</span><span class="mf">0.39</span><span class="p">,</span> <span class="mf">0.61</span><span class="p">,</span> <span class="mf">0.0</span> <span class="p">]]</span>
</pre></div>
</div>
<p>the corresponding uncertainties are</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">uncertainty</span>
<span class="gp">... </span><span class="p">[</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.39</span><span class="p">]</span>
</pre></div>
</div>
<p>In the above example, the most uncertain sample is the second one. When
querying for labels based on this measure, the strategy selects the
sample with the highest uncertainty.</p>
<p>For this ternary classification problem, given the first two
probabilities, the classification uncertainty looks like the following.</p>
<img alt="../../_images/unc-uncertainty.png" class="align-center" src="../../_images/unc-uncertainty.png" />
</div>
<div class="section" id="classification-margin">
<h2>Classification margin<a class="headerlink" href="#classification-margin" title="Permalink to this headline">¶</a></h2>
<p>Classification margin is the difference in probability of the first and second most likely prediction, that is, it is defined by</p>
<div class="math notranslate nohighlight">
\[M(x)=P(\hat{x_1}|x)-P(\hat{x_2}|x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{x_1}\)</span> and <span class="math notranslate nohighlight">\(\hat{x_2}\)</span> are the first and second most likely classes. Using the same example we used for classification uncertainty, if the class probabilities are</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">proba</span>
<span class="gp">... </span><span class="p">[[</span><span class="mf">0.1</span> <span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>
<span class="gp">... </span> <span class="p">[</span><span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.3</span> <span class="p">,</span> <span class="mf">0.1</span> <span class="p">]</span>
<span class="gp">... </span> <span class="p">[</span><span class="mf">0.39</span><span class="p">,</span> <span class="mf">0.61</span><span class="p">,</span> <span class="mf">0.0</span> <span class="p">]]</span>
</pre></div>
</div>
<p>the corresponding margins are</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">margin</span>
<span class="gp">... </span><span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.22</span><span class="p">]</span>
</pre></div>
</div>
<p>When querying for labels, the strategy selects the sample with the <em>smallest</em> margin, since the smaller the decision margin is, the more unsure the decision. In this case, it would be the third sample. For this ternary classification problem, the classifier margin plotted against the first two probabilities are the following.</p>
<img alt="../../_images/unc-margin.png" class="align-center" src="../../_images/unc-margin.png" />
</div>
<div class="section" id="classification-entropy">
<h2>Classification entropy<a class="headerlink" href="#classification-entropy" title="Permalink to this headline">¶</a></h2>
<p>The third built-in uncertainty measure is the classification entropy, which is defined by</p>
<div class="math notranslate nohighlight">
\[H(x)=-\sum_{k}p_k\log(p_k)\]</div>
<p>where <span class="math notranslate nohighlight">\(p_k\)</span> is the probability of the sample belonging to the <em>k</em>-th class. Heuristically, the entropy is proportional to the average number of guesses one has to make to find the true class. In our usual example</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">proba</span>
<span class="gp">... </span><span class="p">[[</span><span class="mf">0.1</span> <span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>
<span class="gp">... </span> <span class="p">[</span><span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.3</span> <span class="p">,</span> <span class="mf">0.1</span> <span class="p">]</span>
<span class="gp">... </span> <span class="p">[</span><span class="mf">0.39</span><span class="p">,</span> <span class="mf">0.61</span><span class="p">,</span> <span class="mf">0.0</span> <span class="p">]]</span>
</pre></div>
</div>
<p>the corresponding entropies are approximately</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">entropy</span>
<span class="gp">... </span><span class="p">[</span><span class="mf">0.5181</span><span class="p">,</span> <span class="mf">0.8979</span><span class="p">,</span> <span class="mf">0.6687</span><span class="p">]</span>
</pre></div>
</div>
<p>The closer the distribution to uniform, the larger the entropy. Again, if we plot the entropy against the first two probabilities of a ternary classification problem, we obtain the following.</p>
<img alt="../../_images/unc-entropy.png" class="align-center" src="../../_images/unc-entropy.png" />
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Disagreement-sampling.html" class="btn btn-neutral float-right" title="Disagreement sampling for committee-based sampling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Acquisition-functions.html" class="btn btn-neutral" title="Acquisition functions for Bayesian optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Tivadar Danka

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>